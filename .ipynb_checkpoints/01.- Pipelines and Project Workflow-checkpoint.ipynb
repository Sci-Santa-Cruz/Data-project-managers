{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"images/logo_anzen_.png\" alt=\"Drawing\" style=\"width:1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines  y   Workflow de un proyecto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Motivación\n",
    "\n",
    "La mayoría de los proyectos de ciencia de datos tienen el mismo conjunto de tareas:\n",
    "\n",
    "1. __ETL__ : extraer datos de su fuente, transformarlos y luego cargarlos en una base de datos. Recuerde, ETL son las siglas de Extract , Transform and Load .\n",
    "2. __Datos de preprocesamiento__: esto puede incluir imputar valores perdidos y elegir los conjuntos de entrenamiento y prueba.\n",
    "3. __Crear características__ : vuelva a combinar y enriquezca los datos para crear características que ayuden al trabajo de modelado.\n",
    "4. __Entrenar modelos__ : puede probar diferentes algoritmos, funciones, etc.\n",
    "5. __Evalúe el rendimiento en el conjunto de pruebas__: utilizando una métrica adecuada (por ejemplo, $Precision$, $Recall$, $AUC$), examine el rendimiento de su modelo \"fuera de la muestra\".\n",
    "6. Piense en cosas nuevas para probar. Repita los pasos 1 a 4 según corresponda.\n",
    "\n",
    "Si el código base no está estructurado y tiene un nombre adecuado, es posible que tenga dificultades para recordar los detalles de cada paso una vez que haya creado algunos modelos. ¿Qué funciones usaste para cada uno? ¿Qué entrenamiento y pruebas se dividen? ¿Qué hiperparámetros?\n",
    "\n",
    "Su código también puede estar desordenado. ¿Sobrescribiste el código del modelo anterior? Quizás copió, pegó y editó código de un modelo anterior. ¿Todavía puedes leer lo que hay allí? Puede convertirse rápidamente en una mezcolanza que requiere heroísmo para descifrar.\n",
    "\n",
    "En esta sesión, presentaremos un flujo de trabajo que puede evitar (o al menos reducir) estos problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipelines\n",
    "\n",
    "Es útil estructurar los datos en varias capas. En las bases de datos, una capa se expresa como esquema. En la mayoría de los demás formatos, se expresan mediante una estructura de directorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw\n",
    "Los datos que recibimos de los _partners_ y _external sources_ es el _Raw_. Los datos brutos son inmutables. Citando del popular _framework_ para creación de _pipelines_  __Data Science Cookiecutter__ :\n",
    "\n",
    "''' Don't ever edit your raw data, especially not manually, and especially not in Excel. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. The code you write should move the raw data through a pipeline to your final analysis. You shouldn't have to run all of the steps every time you want to make a new figure (see Analysis is a DAG), but anyone should be able to reproduce the final products with only the code in src and the data in data/raw. '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capa intermedia\n",
    "Si los datos sin procesar son desordenados, es recomendable crear una capa intermedia que consista en copias ordenadas de los datos sin procesar. Las situaciones típicas en las que esto es útil son:\n",
    "\n",
    "Los datos se reciben en varios tipos de archivos diferentes\n",
    "Los campos de datos no están escritos (por ejemplo, archivos csv, excel) o están mal escritos (fechas como cadenas, formatos de fecha inconsistentes)\n",
    "Los nombres de las columnas no son claros, tienen espacios, caracteres especiales o no hay nombres de columna.\n",
    "Las transformaciones de crudo a intermedio deben limitarse para solucionar los problemas mencionados anteriormente. No debemos combinar diferentes conjuntos de datos ni crear campos calculados. Esto está reservado para la siguiente capa.\n",
    "\n",
    "Los formatos de almacenamiento típicos para la capa intermedia son una base de datos (por ejemplo postgres) o archivos parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Preprocesado\n",
    "Para realizar el trabajo de modelado, los datos de entrada deben combinarse y enriquecerse, por ejemplo, creando características. Los conjuntos de datos que se crean en este proceso se almacenan en la capa procesada. A veces puede resultar útil dividir esta capa en un modelo de datos de dominio, una capa de entidades y una capa maestra, pero la capa exacta dependerá del contexto del proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Modelos\n",
    "\n",
    "Los datos procesados se utilizan para entrenar modelos predictivos, modelos explicativos, motores de recomendación y algoritmos de optimización. Los modelos entrenados se almacenan en la capa del modelo. A diferencia de las capas anteriores, los modelos generalmente se almacenan pickleporque no están en formato tabular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salida del modelo\n",
    "Las métricas de rendimiento del modelo, la información de selección del modelo y las predicciones se mantienen en la capa de salida del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting\n",
    "\n",
    "Los informes se pueden realizar en todo el proceso. Por ejemplo, puede haber informes de calidad de datos sobre las entradas, análisis de distribución de los datos procesados, predicciones, explicaciones, recomendaciones que se brindan al usuario y evaluación y seguimiento del desempeño. Si se construye un front-end, accederá a la capa de informes para mostrar información a los usuarios y desarrolladores. Por ejemplo, un tablero de _Tableau_, _power BI_, un cuaderno _jupyter_ o un resultado de _Excel_ se leerán desde la capa de informes. En consecuencia, el formato de los datos en la capa de informes se ajustará al front-end de su elección.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/data_pipeline.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuración "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El repositorio de código reflejará la canalización de datos creando la estructura de carpetas correspondiente para los archivos de Python.\n",
    "\n",
    "Además, hay muchos otros archivos que deben almacenarse y administrarse. La comunidad ha llegado a una configuración estándar de los directorios del proyecto que también seguiremos.\n",
    "\n",
    "Estructura de directorios:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "├── LICENSE\n",
    "├── README.md          <- The top-level README for developers using this project.\n",
    "├── conf\n",
    "│   ├── base           <- Space for shared configurations like parameters\n",
    "│   └── local          <- Space for local configurations, usually credentials\n",
    "│\n",
    "├── data\n",
    "│   ├── 01_raw         <- Imutable input data\n",
    "│   ├── 02_intermediate<- Cleaned version of raw\n",
    "│   ├── 03_processed   <- The data used for modelling\n",
    "│   ├── 04_models      <- trained models\n",
    "│   ├── 05_model_output<- model output\n",
    "│   └── 06_reporting   <- Reports and input to frontend\n",
    "│\n",
    "├── docs               <- Space for Sphinx documentation\n",
    "│\n",
    "├── notebooks          <- Jupyter notebooks. Naming convention is date YYYYMMDD (for ordering),\n",
    "│                         the creator's initials, and a short '-' delimited description, e.g.\n",
    "│                         '20190601-jqp-initial-data-exploration'.\n",
    "│\n",
    "├── references         <- Data dictionaries, manuals, and all other explanatory materials.\n",
    "│\n",
    "├── results            <- Intermediate analysis as HTML, PDF, LaTeX, etc.\n",
    "│\n",
    "├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n",
    "│                         generated with 'pip freeze > requirements.txt'\n",
    "│\n",
    "├── .gitignore         <- Avoids uploading data, credentials, outputs, system files etc\n",
    "│\n",
    "└── src                <- Source code for use in this project.\n",
    "    ├── __init__.py    <- Makes src a Python module\n",
    "    │\n",
    "    ├── d00_utils      <- Functions used across the project\n",
    "    │   └── remove_accents.py\n",
    "    │\n",
    "    ├── d01_data       <- Scripts to reading and writing data etc\n",
    "    │   └── load_data.py\n",
    "    │\n",
    "    ├── d02_intermediate<- Scripts to transform data from raw to intermediate\n",
    "    │   └── create_int_payment_data.py\n",
    "    │\n",
    "    ├── d03_processing <- Scripts to turn intermediate data into modelling input\n",
    "    │   └── create_master_table.py\n",
    "    │\n",
    "    ├── d04_modelling  <- Scripts to train models and then use trained models to make\n",
    "    │   │                 predictions\n",
    "    │   └── train_model.py\n",
    "    │\n",
    "    ├── d05_model_evaluation<- Scripts that analyse model performance and model selection\n",
    "    │   └── calculate_performance_metrics.py\n",
    "    │    \n",
    "    ├── d06_reporting  <- Scripts to produce reporting tables\n",
    "    │   └── create_rpt_payment_summary.py\n",
    "    │\n",
    "    └── d07_visualisation<- Scripts to create frequently used plots\n",
    "        └── visualise_patient_journey.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flujo de trabajo y experimentación\n",
    "\n",
    "\n",
    "El flujo de trabajo típico para desarrollar código es el siguiente:\n",
    "\n",
    "+ Código de prototipo en un cuaderno jupyter\n",
    "+ Mueva el código a una función que toma datos y parámetros como entradas y devuelve los datos procesados ​​o el modelo entrenado como salida.\n",
    "+ Pruebe la función en el cuaderno jupyter\n",
    "+ Mueva la función a la carpeta src\n",
    "+ Importar la función en el cuaderno jupyter\n",
    "+ Prueba que la función está funcionando\n",
    "\n",
    "Las funciones se pueden importar a un cuaderno de la siguiente manera. Primero le decimos al portátil dónde están las funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "src_dir = os.path.join(os.getcwd(), '..', 'src')\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego indicamos qué funciones importar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from d00_utils.my_fun import my_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código que produce las diferentes capas de la canalización de datos debe abstraerse en funciones.\n",
    "\n",
    "Una canalización de código es un conjunto de código que maneja todas las tareas computacionales que su proyecto necesita de principio a fin. La canalización más simple es un conjunto de funciones encadenadas.\n",
    "\n",
    "Por ejemplo,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#int_data = create_int_data(raw_data)\n",
    "#pro_drug_features = create_pro_drug_features(int_data)\n",
    "#pro_patient_features = create_pro_patient_features(int_data)\n",
    "#pro_master_table = create_pro_master_table(pro_drug_features, pro_patient_features)\n",
    "#model = train_model(pro_master_table)\n",
    "#rpt_report = produce_report(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es un ejemplo muy esquemático. Normalmente, cada paso se divide en varios subconjuntos que crean canalizaciones para cada capa de la canalización de datos. La canalización de un extremo a otro es entonces la concatenación de las subcadenas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1\n",
    "\n",
    "A continuación, se muestra un ejemplo sencillo de una canalización que utiliza `scikit-learnel` utilizando el conjunto de datos de _Boston_:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/boston_pipeline.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "puedes leer la guia completa de `Pipelines` para `scikit-learnel` : https://scikit-learn.org/stable/modules/compose.html#pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta canalización tiene dos pasos. El primero, denominado \"preprocesamiento\", prepara los datos para el modelado mediante la creación de divisiones de entrenamiento y prueba. El segundo, que se denomina \"modelos, predicciones y métricas\", utiliza los datos preprocesados ​​para entrenar modelos, hacer predicciones e imprimir $ R ^ 2 $ en el conjunto de prueba. La tubería toma entradas (por ejemplo, datos, proporciones de entrenamiento / prueba y tipos de modelos) en un extremo y produce salidas ( precisión ) en el otro extremo.\n",
    "\n",
    "Obviamente, este análisis está incompleto, pero el proceso es un buen comienzo. Debido a que usamos el mismo código y datos, podemos ejecutar la canalización de principio a fin y obtener los mismos resultados. Y debido a que dividimos la canalización en funciones, podemos identificar dónde falla la canalización y mejorar la canalización una función a la vez. (Cada función solo necesita usar las mismas entradas y salidas que antes).\n",
    "\n",
    "También tenga en cuenta la función y los bucles en la segunda parte de la canalización. Somos algo agnósticos sobre los métodos que usamos. Si funciona, ¡genial! Esta estructura nos permite recorrer muchos tipos de modelos utilizando los mismos datos preprocesados ​​y las mismas predicciones y métricas. Facilita la adición de nuevos métodos y la comparación de resultados, y nos ayuda a centrarnos en otras partes del proceso, como la generación de funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
