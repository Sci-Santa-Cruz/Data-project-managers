{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines  y   Workflow de un proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines y Flujo de Trabajo del Proyecto\n",
    "\n",
    "## Motivación\n",
    "\n",
    "La mayoría de los proyectos de ciencia de datos tienen el mismo conjunto de tareas:\n",
    "\n",
    "1. **ETL**: extraer datos de su fuente, transformarlos y luego cargarlos en una base de datos. Recuerda, **ETL** significa *Extraer*, *Transformar* y *Cargar*.\n",
    "2. **Enlace**: combinar datos procedentes de diferentes fuentes (también conocido como enlace de registros o coincidencia).\n",
    "3. **Preprocesar datos**: Esto podría incluir la limpieza y manipulación de datos.\n",
    "4. **Crear una serie de conjuntos de entrenamiento y validación**.\n",
    "5. **Crear características**: Recombinar y enriquecer los datos para crear características que ayuden en el trabajo de modelado, así como explorar opciones de imputación.\n",
    "6. **Entrenar el/los modelo(s)**: Puedes probar diferentes algoritmos, características, y demás.\n",
    "7. **Selección de modelos**: Evaluar el rendimiento en los conjuntos de validación: Utilizando una métrica adecuada (por ejemplo, $Precisión@k$, $Recall$, $AUC$), examinar el rendimiento de tu modelo \"fuera de muestra\" basado en la tarea y el contexto de implementación en métricas adecuadas (incluyendo sesgo, precisión, etc.).\n",
    "8. Pensar en nuevas cosas para probar. Repetir los pasos 1 al 4 según corresponda.\n",
    "\n",
    "Si la base de código no está estructurada adecuadamente, podrías tener dificultades para recordar los detalles de cada paso una vez que hayas construido algunos modelos. ¿Qué características usaste para cada uno? ¿Qué divisiones de entrenamiento y validación? ¿Qué hiperparámetros?\n",
    "\n",
    "Tu código también podría estar volviéndose desordenado. ¿Sobrescribiste el código del modelo anterior? Tal vez copiaste, pegaste y editaste código de un modelo anterior. ¿Todavía puedes entender lo que hay ahí? Rápidamente puede convertirse en una mezcolanza que requiere heroísmos para descifrar.\n",
    "\n",
    "En esta sesión, presentaremos un flujo de trabajo que puede evitar (o al menos reducir) estos problemas.\n",
    "\n",
    "# Tuberías de Datos\n",
    "\n",
    "Es útil estructurar los datos en múltiples capas.\n",
    "En bases de datos, una capa se expresa como esquema.\n",
    "En la mayoría de los otros formatos, se expresan a través de una estructura de directorio.\n",
    "\n",
    "##### Crudo\n",
    "Los datos que recibimos de los socios y fuentes externas son los datos crudos.\n",
    "Los datos crudos son inmutables. Citando del popular paquete de flujo de trabajo [Data Science Cookiecutter](https://drivendata.github.io/cookiecutter-data-science/#data-is-immutable):\n",
    "\n",
    "Nunca edites tus datos crudos, especialmente no manualmente y especialmente no en Excel.\n",
    "No sobrescribas tus datos crudos.\n",
    "No guardes múltiples versiones de los datos crudos.\n",
    "Trata los datos (y su formato) como inmutables.\n",
    "El código que escribas debe mover los datos crudos a través de una tubería hacia tu análisis final.\n",
    "No deberías tener que ejecutar todos los pasos cada vez que quieras crear una nueva figura (ver Análisis es un DAG), pero cualquiera debería poder reproducir los productos finales con solo el código en src y los datos en data/raw.\n",
    "\n",
    "\n",
    "##### Intermedio\n",
    "Si los datos crudos son desordenados, es aconsejable crear una capa intermedia que consista en copias ordenadas de los datos crudos.\n",
    "Situaciones típicas en las que esto es útil son:\n",
    "- Se reciben datos en múltiples tipos de archivos diferentes.\n",
    "- Los campos de datos no están tipados (por ejemplo, archivos csv, Excel) o están mal tipados (fechas como cadenas, formatos de fecha inconsistentes).\n",
    "- Los nombres de las columnas no son claros, tienen espacios, caracteres especiales o no tienen nombres de columna.\n",
    "\n",
    "Las transformaciones de crudo a intermedio deben limitarse a corregir los problemas mencionados anteriormente. No debemos combinar diferentes conjuntos de datos ni crear campos calculados. Esto está reservado para la próxima capa.\n",
    "\n",
    "Los formatos de almacenamiento típicos para la capa intermedia son una base de datos (por ejemplo, `postgres`) o archivos `parquet`.\n",
    "\n",
    "##### Procesado\n",
    "Para realizar el trabajo de modelado, los datos de entrada deben combinarse y enriquecerse, por ejemplo, mediante la creación de características.\n",
    "Los conjuntos de datos creados en este proceso se almacenan en la capa procesada.\n",
    "A veces puede ser útil dividir esta capa en un modelo de datos de dominio, una capa de características y una capa maestra, pero la estratificación exacta dependerá del contexto del proyecto.\n",
    "\n",
    "##### Modelos\n",
    "Los datos procesados se utilizan para entrenar modelos predictivos, modelos explicativos, motores de recomendación y algoritmos de optimización.\n",
    "Los modelos entrenados se almacenan en la capa de modelos. \n",
    "A diferencia de las capas anteriores, los modelos generalmente se almacenan en `pickle` porque no están en formato tabular.\n",
    "\n",
    "##### Salida del Modelo\n",
    "Las métricas de rendimiento del modelo, la información de selección de modelos y las predicciones se mantienen en la capa de salida del modelo.\n",
    "\n",
    "##### Reportes\n",
    "La generación de informes se puede realizar en todo el conducto.\n",
    "Por ejemplo, puede haber informes de calidad de datos sobre las entradas, análisis de distribución sobre los datos procesados, predicciones, explicaciones, recomendaciones que se proporcionan al usuario, y evaluación y seguimiento del rendimiento.\n",
    "Si se construye una interfaz de usuario, accederá a la capa de informes para mostrar información a los usuarios y desarrolladores.\n",
    "Por ejemplo, un panel de Tableau, Power BI, un cuaderno Jupyter o una salida de Excel leerán desde la capa de informes. En consecuencia, el formato de los datos en la capa de informes se ajustará a la elección de la interfaz.\n",
    "\n",
    "<img src=\"images/data_pipeline.png\" width=\"90%\">\n",
    "\n",
    "\n",
    ".png \"Data pipeline\")\n",
    "\n",
    "# Configuración del Código\n",
    "El repositorio de código reflejará la tubería de datos al crear la estructura de carpetas correspondiente para los archivos de Python.\n",
    "\n",
    "Además, hay varios otros archivos que deben almacenarse y gestionarse.\n",
    "La comunidad ha llegado a una configuración estándar de los directorios del proyecto que también seguiremos.\n",
    "\n",
    "Estructura de directorio:\n",
    "\n",
    "```\n",
    "├── LICENSE\n",
    "├── README.md          <- README principal para los desarrolladores que utilizan este proyecto.\n",
    "├── conf\n",
    "│   ├── base           <- Espacio para configuraciones compartidas como parámetros.\n",
    "│   └── local          <- Espacio para configuraciones locales, generalmente credenciales.\n",
    "│\n",
    "├── data\n",
    "│   ├── 01_raw         <- Datos de entrada inmutables\n",
    "│   ├── 02_intermediate<- Versión limpia de los datos crudos\n",
    "│   ├── 03_processed   <- Los datos utilizados para el modelado\n",
    "│   ├── 04_models      <- Modelos entrenados\n",
    "│   ├── 05_model_output<- Salida del modelo\n",
    "│   └── 06_reporting   <- Informes y entrada para el frontend\n",
    "│\n",
    "├── docs               <- Espacio para la documentación de Sphinx\n",
    "│\n",
    "├── notebooks          <- Cuadernos Jupyter. La convención de nombres es la fecha en formato YYYYMMDD (para ordenar), las iniciales del creador y una breve descripción separada por `-`, por ejemplo, `20190601-jqp-exploracion-inicial-de-datos`.\n",
    "│\n",
    "├── references         <- Diccionarios de datos, manuales y otros materiales explicativos.\n",
    "│\n",
    "├── results            <- Análisis intermedio en formatos como HTML, PDF, LaTeX, etc.\n",
    "│\n",
    "├── requirements.txt   <- Archivo de requisitos para reproducir el entorno de análisis, generado con `pip freeze > requirements.txt`.\n",
    "│\n",
    "├── .gitignore         <- Evita cargar datos, credenciales, salidas, archivos del sistema, etc.\n",
    "│\n",
    "└── src                <- Código fuente para su uso en este proyecto.\n",
    "    ├── __init__.py    <- Hace que src sea un módulo de Python\n",
    "    │\n",
    "    ├── d00_utils      <- Funciones utilizadas en todo el proyecto\n",
    "    │   └── remove_accents.py\n",
    "    │\n",
    "    ├── d01_data       <- Scripts para lectura y escritura de datos, etc.\n",
    "    │   └── load_data.py\n",
    "    │\n",
    "    ├── d02_intermediate<- Scripts para transformar datos de crudos a intermedios\n",
    "    │   └── create_int_payment_data.py\n",
    "    │\n",
    "    ├── d03_processing <- Scripts para convertir datos intermedios en entradas de modelado\n",
    "    │   └── create_master_table.py\n",
    "    │\n",
    "    ├── d04_modelling  <- Scripts para entrenar modelos y luego usar modelos entrenados para hacer predicciones\n",
    "    │   │\n",
    "    │   └── train_model.py\n",
    "    │\n",
    "    ├── d05_model_evaluation<- Scripts que analizan el rendimiento del modelo y la selección de modelos\n",
    "    │   └── calculate_performance_metrics.py\n",
    "    │    \n",
    "    ├── d06_reporting  <- Scripts para producir tablas de informes\n",
    "    │   └── create_rpt_payment_summary.py\n",
    "    │\n",
    "    └── d06_visualisation<- Scripts para crear gráficos de uso frecuente\n",
    "        └── visualise_patient_journey.py\n",
    "```\n",
    "\n",
    "# Flujo de Trabajo\n",
    "\n",
    "El flujo de trabajo típico para desarrollar código es el siguiente:\n",
    "\n",
    "- Prototipo de código en un cuaderno Jupyter.\n",
    "- Mueva el código a una función que tome datos y parámetros como entradas y devuelva los datos procesados o el modelo entrenado como salida.\n",
    "- Prueba la función en el cuaderno Jupyter.\n",
    "- Mueva la función a la carpeta `src`.\n",
    "- Importa la función en el cuaderno Jupyter.\n",
    "- Prueba que la función esté funcionando.\n",
    "\n",
    "Las funciones se pueden importar en un cuaderno de la siguiente manera. Primero, indicamos al cuaderno dónde se encuentran las funciones:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "src_dir = os.path.join(os.getcwd(), '..', 'src')\n",
    "sys.path.append(src_dir)\n",
    "```\n",
    "\n",
    "Luego, especificamos qué funciones importar:\n",
    "\n",
    "```python\n",
    "from d00_utils.my_fun import my_fun\n",
    "```\n",
    "\n",
    "¡Pruébalo!\n",
    "\n",
    "# Tubería de Código\n",
    "\n",
    "El código que produce las diferentes capas de la tubería de datos debe abstraerse en funciones.\n",
    "\n",
    "Una *tubería de código* es un conjunto de código que maneja todas las tareas computacionales que necesita tu proyecto desde el principio hasta el final. La tubería más simple es un conjunto de funciones encadenadas.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "```python\n",
    "int_data = create_int_data(raw_data)\n",
    "pro_drug_features = create_pro_drug_features(int_data)\n",
    "pro_patient_features = create_pro_patient_features(int_data)\n",
    "pro_master_table = create_pro_master_table(pro_drug_features, pro_patient_features)\n",
    "model = train_model(pro_master_table)\n",
    "rpt_report = produce_report(model)\n",
    "```\n",
    "\n",
    "Este es un ejemplo muy esquemático. Normalmente, cada paso se divide en varios subconjuntos creando tuberías para cada capa de la tubería de datos. La tubería de extremo a extremo es la concatenación de las sub-tuberías.\n",
    "\n",
    "## Ejemplos\n",
    "\n",
    "#### Ejemplo 1\n",
    "Aquí tienes un ejemplo simple de una tubería que utiliza el conjunto de datos de Boston de `scikit-learn`:\n",
    "\n",
    "<img src=\"images/boston_pipeline.png\" width=\"100%\">\n",
    "\n",
    "Esta tubería tiene dos pasos. El primero, llamado \"preprocesamiento\", prepara los datos para el modelado creando divisiones de entrenamiento y prueba. El segundo, llamado \"modelos, predicciones y métricas\", utiliza los datos preprocesados para entrenar modelos, hacer predicciones e imprimir $R^2$ en el conjunto de prueba. La tubería toma entradas (por ejemplo, datos, proporciones de entrenamiento/prueba y tipos de modelo) en un extremo y produce salidas (precisión, por ejemplo) en el otro extremo.\n",
    "\n",
    "Obviamente, este análisis está incompleto, pero la tubería es un buen comienzo. Dado que usamos el mismo código y datos, podemos ejecutar la tubería de principio a fin y obtener los mismos resultados. Y como dividimos la tubería en funciones, podemos identificar dónde falla la tubería y mejorarla una función a la vez. (Cada función solo necesita usar las mismas entradas y salidas que antes).\n",
    "\n",
    "También observa la función y los bucles en la segunda parte de la tubería. Somos algo agnósticos sobre los métodos que usamos. Si funciona, ¡\n",
    "\n",
    "genial! Esta estructura nos permite recorrer muchos tipos de modelos utilizando los mismos datos preprocesados y las mismas predicciones y métricas. Facilita la adición de nuevos métodos y la comparación de resultados, y nos ayuda a concentrarnos en otras partes de la tubería, como la generación de características.\n",
    "\n",
    "#### Ejemplo 2\n",
    "\n",
    "La [tubería de la policía](https://github.com/dssg/police-eis), iniciada en [**DSSG 2015**](https://dssg.uchicago.edu/people/2015-fellows-mentors/), es un ejemplo de una tubería relativamente bien desarrollada. Nos permite especificar las opciones de la tubería que deseamos en un archivo yaml, desde el preprocesamiento en adelante. (El código en este repositorio no incluye ETL). Nos proporciona muchas opciones de modelado y facilita las comparaciones.\n",
    "\n",
    "#### Ejemplo 3\n",
    "\n",
    "Echa un vistazo a [Triage](http://github.com/dssg/triage), una herramienta que hemos creado para respaldar proyectos similares a DSSG.\n",
    "\n",
    "## Recursos\n",
    "\n",
    "* [Nuestra tubería principal](https://github.com/dssg/lead-public), iniciada en DSSG 2014\n",
    "* [Nuestra tubería de Cincinnati](https://github.com/dssg/cincinnati), iniciada en DSSG 2015\n",
    "* [Triage](https://github.com/dssg/triage) (una tubería DSSG generalizada)\n",
    "* [https://scikit-learn.org/stable/modules/compose.html#pipeline]( puedes leer la guia completa de `Pipelines` para `scikit-learnel`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integración de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener y conservar datos\n",
    "Los datos vienen en muchas formas, de muchas fuentes: es posible que obtenga un volcado de base de datos directamente de un socio del proyecto, o puede que necesite extraer datos de la web (consulte Web Scraping básico ). De cualquier manera, una vez que tenga algunos datos en sus manos, deberá incorporarlos a una base de datos y comenzar a formatearlos de tal manera que pueda usarlos para el análisis. Las herramientas de línea de comandos empezarán a resultar útiles aquí. Si sus datos están en un formato similar, CSV estas instrucciones le resultarán útiles . Definitivamente querrá realizar un seguimiento de los pasos que siguió para pasar de datos sin procesar a datos listos para el modelo ( ETL reproducible ).\n",
    "\n",
    "A menudo, los proyectos de ciencia de datos para el bien social involucran datos confidenciales, por lo que es importante conocer algunos principios básicos de seguridad de datos: Introducción a la seguridad de datos ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Reproducible\n",
    "\n",
    "## Motivación\n",
    "\n",
    "- **Entender lo que hiciste**: Guardar todos los pasos que tomaste para que puedas decir cómo llegaste aquí.\n",
    "- **Usar lo que hiciste**: Usar y reutilizar el código para este proyecto o para otros. Corregir errores fácilmente. Importar nuevos datos con confianza. ¡HÁZLO IMPLEMENTADO!\n",
    "\n",
    "Esta sesión se basa en lo que aprendiste la semana pasada en la [sesión CSV a DB](https://github.com/dssg/hitchhikers-guide/tree/master/curriculum/1_getting_and_keeping_data/csv-to-db).\n",
    "\n",
    "## Posibles Enseñanzas\n",
    "\n",
    "- Transferencias a bases de datos propietarias (por ejemplo, SQL Server, Oracle).\n",
    "\n",
    "## Conceptos\n",
    "\n",
    "Muchas personas asocian la ciencia de datos con algoritmos avanzados de aprendizaje automático, pero la ETL (Extracción, Transformación y Carga) es argumentablemente más importante.\n",
    "\n",
    "ETL:\n",
    "\n",
    "1. **Extracción**: Obtener datos de la fuente, por ejemplo, un CSV que te proporcionó un socio.\n",
    "2. **Transformación**: Llevar los datos al formato que deseas/necesitas, por ejemplo, estandarizar los valores faltantes.\n",
    "3. **Carga**: Introducir los datos en la base de datos.\n",
    "\n",
    "Existen dos razones por las que la ETL es tan importante:\n",
    "\n",
    "1. El resto de tu análisis depende de tu ETL. Por ejemplo, [puedes ignorar algunos de los casos más importantes si eliminas filas con valores faltantes](http://www.stabilityjournal.org/articles/10.5334/sta.cr/).\n",
    "\n",
    "2. Datos de mejor calidad pueden ayudar más que métodos de mejor calidad.\n",
    "\n",
    "Por lo tanto, debes hacer la ETL de la siguiente manera:\n",
    "\n",
    "- Confiabilidad.\n",
    "- Comprensibilidad.\n",
    "- Preferiblemente de forma automática.\n",
    "\n",
    "Herramientas:\n",
    "\n",
    "- El código es ~~típicamente~~ mejor que las interfaces gráficas de usuario (GUI). --- El código se puede automatizar.\n",
    "- En igualdad de condiciones, las herramientas de línea de comandos son buenas opciones. Han sido probadas con el tiempo y son eficientes.\n",
    "- `make` (una herramienta de línea de comandos escrita para compilar software eficientemente).\n",
    "- Si no puedes guardar el código, guarda las notas, por ejemplo, registra cómo utilizaste Pentaho para transferir una base de datos Oracle a PostgreSQL.\n",
    "\n",
    "\n",
    "### Pruebas ETL\n",
    "\n",
    "Corres el riesgo de perder o corromper datos en cada paso. Para asegurarte de que extrajiste, transformaste y cargaste los datos correctamente, puedes realizar comprobaciones simples. Aquí tienes algunas formas:\n",
    "\n",
    "- Si estás copiando un archivo, verifica el hash antes y después. Deberían coincidir.\n",
    "- Cuenta las filas. Si deben coincidir antes y después, ¿lo hacen?\n",
    "- Verifica estadísticas agregadas, como la suma de una columna. Si deben coincidir antes y después, ¿lo hacen?\n",
    "- Si recibes una copia de base de datos, cuenta el número de esquemas/tablas/secuencias, etc., en las bases de datos de origen y destino. ¿Coinciden? Por ejemplo, solicitamos a los socios que utilizan Oracle que ejecuten el script `count_rows_oracle.sql`, que proporciona el número de filas y columnas en la base de datos de origen. Es una forma (necesaria pero no suficiente) de comprobar que obtuvimos todos los datos.\n",
    "\n",
    "\n",
    "### ¿Qué pasa si tienes que usar interfaces gráficas de usuario?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias \n",
    "[Esté documento es una adaptación de SSD ](https://github.com/dssg/hitchhikers-guide/tree/master/sources/curriculum/0_before_you_start/pipelines-and-project-workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
