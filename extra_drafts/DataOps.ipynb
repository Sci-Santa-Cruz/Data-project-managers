{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de adoptar DataOps como solución, es importante comprender el problema que estamos tratando de resolver. Cuando ve artículos en línea, escucha presentaciones en conferencias o lee sobre el éxito de organizaciones líderes basadas en datos como _Facebook_ , _Amazon_ , _Netflix_ y _Google (FANG)_ , entregar ciencia de datos exitosa parece un proceso simple. La realidad es muy diferente. Si bien, sin duda, existen historias de éxito, también hay mucha evidencia de que una inversión sustancial en ciencia de datos no está generando los retornos esperados para la mayoría de las organizaciones. Hay múltiples causas, pero se derivan de dos causas fundamentales. Primero, un enfoque de arquitectura de la información del siglo XX para manejar datos y análisis en el siglo XXI. En segundo lugar, la falta de conocimiento y apoyo organizacional para la ciencia y el análisis de datos. Los mantras comunes (del siglo XX) adoptados en la industria para superar estos problemas empeoran las cosas, no mejoran."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Hay algún problema?\n",
    "\n",
    "Es posible crear una ventaja competitiva y resolver problemas valiosos utilizando datos. Muchas organizaciones están logrando generar historias de éxito legítimas a partir de sus inversiones en ciencia de datos y análisis de datos:\n",
    "- El vicepresidente de innovación de productos, Carlos Uribe-Gómez, y el director de productos, Neil Hunt, publicaron un documento que dice que algunos de sus algoritmos de recomendación le ahorran a Netflix mil millones de dólares cada año en una reducción de la rotación.\n",
    "- Una de las iniciativas de ciencia de datos de Monsanto para mejorar el transporte y la logística global ofrece ahorros anuales y reducción de costos de casi $ 14 millones, al tiempo que reduce las emisiones de C02 en 350 toneladas métricas (TM).\n",
    "- DeepMind de Alphabet, mejor conocido por su programa AlphaGo, ha desarrollado un sistema de inteligencia artificial (IA) en asociación con el Moorfield Eye Hospital de Londres para derivar el tratamiento de más de 50 enfermedades que amenazan la vista con tanta precisión como los médicos expertos líderes en el mundo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "No queriendo quedarse atrás, la mayoría de las organizaciones ahora están gastando mucho en tecnología costosa y contratando costosos equipos de científicos de datos, ingenieros de datos y analistas de datos para dar sentido a sus datos e impulsar decisiones. Lo que alguna vez fue una actividad de nicho incluso en las organizaciones más grandes, ahora se considera una competencia central. Las tasas de crecimiento de la inversión y la posición laboral son asombrosas considerando que el PIB mundial solo está creciendo a un 3,5% anual:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "International Data Corp. (IDC) espera que los ingresos mundiales por big data y soluciones de análisis empresarial alcancen los 260.000 millones de dólares en 2022, una tasa de crecimiento anual compuesta del 11,9% durante el período 2017-2022.4\n",
    "Los informes de empleos emergentes de LinkedIn clasifican a los ingenieros de aprendizaje automático, científicos de datos e ingenieros de big data como tres de los cuatro empleos de mayor crecimiento en los Estados Unidos entre 2012 y 2017. ¡Los roles de científicos de datos aumentaron más del 650% durante ese período!(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LA REALIDAD\n",
    "\n",
    "A pesar del enorme desembolso monetario, solo una minoría de organizaciones logra resultados significativos. Los estudios de casos que demuestran resultados cuantificables son excepciones aisladas, que incluso permiten la renuencia a revelar ventajas competitivas. El crecimiento exponencial en el volumen de datos, los rápidos aumentos en el gasto en soluciones y las mejoras en tecnología y algoritmos no han llevado a un aumento en la productividad del análisis de datos.\n",
    "Hay indicios de que la tasa de éxito de los proyectos de análisis de datos está disminuyendo. En 2016, Forrester concluyó que solo el 22% de las empresas experimentaron un alto crecimiento de los ingresos y se beneficiaron de sus inversiones en ciencia de datos.6Además, en 2016, Gartner estimó que el 60% de los proyectos de big data fracasan, pero empeora. \n",
    "\n",
    "En 2017, Nick Heudecker de Gartner emitió una corrección. La estimación del 60% fue \"demasiado conservadora\", la tasa real de fallas estuvo más cerca del 85%.7Aunque muchos de los datos de la encuesta están relacionados con “big data”, sigo pensando que los resultados de Nick son relevantes. Fuera del campo de la ciencia de datos, la mayoría de la gente piensa erróneamente en big data, ciencia de datos y análisis de datos como términos intercambiables y responderán como tales.\n",
    "\n",
    "Puede haber múltiples razones para la escasa tasa de rendimiento y la imposibilidad de mejorar la productividad a pesar de una importante inversión en ciencia y análisis de datos. El crecimiento explosivo en la captura de datos puede resultar en la adquisición de datos de valor marginal cada vez más bajos. Es posible que la tecnología, las bibliotecas de software y los algoritmos no sigan el ritmo del volumen y la complejidad de los datos capturados. Los niveles de habilidad de los científicos de datos podrían ser insuficientes. \n",
    "\n",
    "Es posible que los procesos no estén evolucionando para aprovechar las oportunidades basadas en datos. Por último, las barreras organizativas y culturales podrían estar impidiendo la explotación de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor y la calidad de los datos\n",
    "\n",
    "No hay indicios de que el valor marginal de los datos recopilados haya disminuido. Gran parte de los datos adicionales capturados provienen cada vez más de fuentes nuevas, como sensores de dispositivos o dispositivos móviles de Internet de las cosas (IoT) , datos no estructurados y documentos de texto, imágenes o semiestructurados generados por registros de eventos. \n",
    "\n",
    "El mayor volumen y variedad de datos adquiridos está ampliando la oportunidad para que los científicos de datos extraigan conocimiento e impulsen decisiones.Sin embargo, existe evidencia de que la mala calidad de los datos sigue siendo un desafío importante. En el Informe de científicos de datos 2018 de Figure Eight, el 55% de los científicos de datos mencionaron la calidad / cantidad de los datos de entrenamiento como su mayor desafío.8 La tasa había cambiado poco desde el informe inaugural de 2015, cuando el 52,3% de los científicos de datos citaron datos de mala calidad como su mayor obstáculo diario.9\n",
    "\n",
    "Los datos sucios también se citaron como la barrera número uno en la Encuesta sobre el estado de los datos y el aprendizaje automático de Kaggle de 2017 a 16,000 encuestados, mientras que \"datos no disponibles o de difícil acceso\" fue la quinta barrera más significativa y mencionada por el 30,2% de los encuestados. 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tecnología, software y algoritmos\n",
    "\n",
    "No hay indicios de que la tecnología, las bibliotecas de software y los algoritmos no estén a la altura del volumen y la complejidad de los datos capturados. Las bibliotecas de tecnología y software continúan evolucionando para manejar problemas cada vez más desafiantes al tiempo que agregan interfaces simplificadas para ocultar la complejidad a los usuarios o aumentar la automatización. Donde antes la ejecución de un clúster de Hadoop en las instalaciones era la única opción para trabajar con varios terabytes de datos, ahora las mismas cargas de trabajo se pueden ejecutar en motores de consultas Spark o SQL administrados como servicio en la nube sin ingeniería de infraestructura. requisito.Las bibliotecas de software como Keras facilitan mucho el trabajo con bibliotecas de aprendizaje profundo como el popular TensorFlow de Google. Proveedores como DataRobot han automatizado la producción de modelos de aprendizaje automático. Los avances en los algoritmos y arquitecturas de aprendizaje profundo y las grandes redes neuronales con muchas capas, como las redes neuronales convolucionales (CNN) y las redes de memoria a corto plazo (redes LSTM), han permitido un cambio radical en el procesamiento del lenguaje natural (NLP), traducción automática, reconocimiento de imágenes, procesamiento de voz y análisis de video en tiempo real . En teoría, todos estos desarrollos deberían mejorar la productividad y el retorno de la inversión.(ROI) de la inversión en ciencia de datos. Quizás las organizaciones estén usando tecnología obsoleta o incorrecta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Científicos de datos\n",
    "\n",
    "Como campo relativamente nuevo, la inexperiencia de los científicos de datos puede ser un problema. En la Encuesta sobre el estado de los datos y el aprendizaje automático de Kaggle, el rango de edad modal de los científicos de datos era solo de 24 a 26 años, y la edad media era de 30. La edad media variaba según el país; para los Estados Unidos, fue 32. Sin embargo, esto es todavía mucho más bajo que la edad media del trabajador estadounidense a los 41 años. Sin embargo, el nivel educativo no fue un problema, el 15,6% tenía un doctorado, el 42% tenía una maestría y el 32% una licenciatura. 10 Dado que todas las formas de análisis avanzado eran marginales antes de 2010, también hay una deficiencia de gerentes experimentados. Como resultado, tenemos muchos científicos de datos extremadamente brillantes con poca experiencia en el manejo de la cultura organizacional. y falta de liderazgo analítico superior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesos de ciencia de datos\n",
    "\n",
    "Es un desafío encontrar datos de encuestas sobre los procesos y metodologías utilizados para entregar ciencia de datos. La encuesta de KDnuggets de 2014 mostró que el proceso estándar de la industria para la minería de datos (CRISP-DM) es la principal metodología para proyectos de análisis, minería de datos y ciencia de datos utilizada por el 43% de los encuestados. 11 \n",
    "\n",
    "El siguiente enfoque más popular no fue un método en absoluto, sino los encuestados que siguieron su propio proceso. El modelo Sample, Explore, Modify, Model and Assess (SEMMA) del Instituto SAS ocupó el tercer lugar, pero en rápido declive ya que el uso está estrechamente vinculado a los productos SAS.El desafío con CRISP-DM y otras metodologías de minería de datos como Knowledge Discovery Databases  (KDD) es que tratan la ciencia de datos como un proceso mucho más lineal de lo que es. Animan a los científicos de datos a dedicar mucho tiempo a planificar y analizar para una única entrega casi perfecta, que puede no ser lo que el cliente desea en última instancia. No se centra la atención en el producto mínimo viable, los comentarios de los clientes o la iteración para asegurarse de que está dedicando el tiempo sabiamente a trabajar en lo correcto. \n",
    "\n",
    "También tratan la implementación y el monitoreo como un problema de \"tirar por la borda\", donde el trabajo se pasa a otros equipos para su finalización con poca comunicación o colaboración, lo que reduce las posibilidades de una entrega exitosa.En respuesta, muchos grupos han propuesto nuevas metodologías, incluido Microsoft con su Proceso de ciencia de datos en equipo  (TDSP) . 12  TDSP es una mejora significativa con respecto a los enfoques anteriores y reconoce que la entrega de la ciencia de datos debe ser ágil, iterativa, estandarizada y colaborativa. Desafortunadamente, el TDSP no parece estar ganando mucha tracción. \n",
    "\n",
    "TDSP y metodologías similares también están restringidas al ciclo de vida de la ciencia de datos. Existe la oportunidad de una metodología que abarque el ciclo de vida de los datos de un extremo a otro, desde la adquisición hasta la jubilación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cultura organizacional\n",
    "\n",
    "Los factores emocionales, situacionales y culturales influyen en gran medida en las decisiones comerciales. La encuesta de FORTUNE Knowledge Group a más de 700 ejecutivos de alto nivel de una variedad de disciplinas en nueve industrias demuestra las barreras para la toma de decisiones basada en datos. La mayoría (61%) de los ejecutivos está de acuerdo en que, al tomar decisiones, las percepciones humanas deben preceder al análisis duro. \n",
    "\n",
    "El sesenta y dos por ciento de los encuestados afirma que a menudo es necesario confiar en los \"sentimientos viscerales\" y que los factores suaves deben tener el mismo peso que los factores duros. Es preocupante que dos tercios (66%) de los ejecutivos de TI dicen que las decisiones a menudo se toman por el deseo de ajustarse a \"la forma en que siempre se han hecho las cosas\". 13Estos no son hallazgos aislados. La Encuesta Ejecutiva de Big Data 2017 de NewVantage Partners encontró que los desafíos culturales siguen siendo un impedimento para la adopción empresarial exitosa:Más del 85% de los encuestados informan que sus empresas han iniciado programas para crear culturas basadas en datos, pero solo el 37% informa haber tenido éxito hasta ahora. La tecnología de big data no es el problema; la comprensión de la gestión, la alineación organizacional y la resistencia organizacional general son los culpables. Si las personas fueran tan maleables como los datos. 14No es de extrañar que muy pocas empresas hayan seguido el ejemplo de Amazon y hayan reemplazado a los tomadores de decisiones de cuello blanco altamente remunerados con algoritmos a pesar del enorme éxito que ha logrado. 15\n",
    "\n",
    "El desafío de entregar una ciencia de datos exitosa tiene mucho menos que ver con la tecnología, sino con la actitud cultural en la que muchas organizaciones tratan la ciencia de datos alternativamente como un ejercicio de marcar casillas o como parte de la búsqueda interminable de la solución perfecta para todos sus desafíos. El problema tampoco es la eficacia de los algoritmos. Los algoritmos y la tecnología están muy por delante de nuestra capacidad para alimentarlos con datos de alta calidad, superar las barreras de las personas (habilidades, cultura y organización) e implementar procesos centrados en datos. Sin embargo, estos síntomas son en sí mismos el resultado de causas profundas más profundas, como la falta de conocimiento de la mejor manera de usar los datos para tomar decisiones, las percepciones heredadas del enfoque del siglo pasado para manejar datos y brindar análisis, y la escasez de soporte para el análisis de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La brecha del conocimiento\n",
    "\n",
    "Las múltiples lagunas de conocimiento dificultan la integración de la ciencia de datos en las organizaciones cuando la implementación comienza en la cima de una organización. Sin embargo, es demasiado fácil culpar a los líderes empresariales y a los profesionales de TI por su incapacidad para generar resultados. La brecha de conocimiento es una vía de doble sentido porque los científicos de datos deben compartir la culpa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La brecha de conocimiento de los científicos de datos\n",
    "\n",
    "La ciencia de datos tiene como objetivo facilitar mejores decisiones, lo que lleva a acciones beneficiosas al extraer conocimiento de los datos. Para permitir mejores decisiones, los científicos de datos necesitan una buena comprensión del dominio empresarialpara permitir una mejor comprensión del problema comercial, identificar los datos correctos y prepararlos (a menudo detectando problemas de calidad por primera vez), emplear los algoritmos correctos en los datos, validar sus enfoques, convencer a las partes interesadas para que actúen, operacionalizar sus resultados y medir resultados.\n",
    "\n",
    "Esta amplitud de alcance requiere una amplia gama de habilidades, como la capacidad de colaborar y coordinarse con múltiples funciones dentro de la organización, además de su propia área de trabajo, pensamiento crítico y científico, habilidades de codificación y desarrollo de software, y conocimiento de una amplia gama de aprendizaje automático y algoritmos estadísticos. Además, la capacidad de comunicar ideas complejas a una audiencia no técnica y perspicacia para los negocios en un entorno comercial es crucial. En la profesión de la ciencia de datos,Dado que encontrar un unicornio es raro, si no imposible (no se registran en LinkedIn ni asisten a reuniones), las organizaciones intentan encontrar la mejor opción. Contratan personas con habilidades de programación (Python o R), análisis, aprendizaje automático y estadísticas y ciencias de la computación, que son las cinco habilidades más buscadas por los empleadores. 16 \n",
    "\n",
    "Estas habilidades deberían ser el diferenciador entre los científicos de datos y todos los demás. Desafortunadamente, esta creencia refuerza la convicción errónea entre los científicos de datos jóvenes de que las habilidades técnicas especializadas deberían ser el foco, pero esto tiende a crear equipos peligrosamente homogéneos.\"Créame un modelo de atención de traducción automática utilizando una memoria bidireccional a largo plazo a corto plazo (LSTM) con una capa de atención que se envía a un LSTM posterior a la atención apilado que alimenta una capa softmax para las predicciones\", dijo ningún CEO.\n",
    "\n",
    "Siempre me sorprende la cantidad de candidatos que dicen que su objetivo principal es un rol que les permite crear modelos de aprendizaje profundo, aprendizaje reforzado y [inserte su algoritmo aquí] siempre que me entrevistan. \n",
    "__Su objetivo no es resolver un problema real o ayudar a los clientes, sino aplicar la técnica más actual__. \n",
    "\n",
    "La ciencia de datos es demasiado valiosa para tratarla como un pasatiempo pagado.Existe una desconexión entre las habilidades que los científicos de datos creen que necesitan y lo que realmente necesitan.\n",
    "\n",
    "\n",
    "Desafortunadamente, las habilidades técnicas no son lo suficientemente cerca como para impulsar el éxito real y las acciones beneficiosas a partir de decisiones basadas en datos. Enfrentados a datos de mala calidad de difícil acceso, falta de apoyo administrativo, sin preguntas claras que responder o resultados ignorados por los responsables de la toma de decisiones, los científicos de datos sin liderazgo sénior en ciencia de datos no están equipados para cambiar la cultura. Algunos buscan pastos más verdes y consiguen un nuevo trabajo, solo para darse cuenta de que existen desafíos similares en la mayoría de las organizaciones. Otros se centran en la parte del proceso que pueden controlar, el modelado.\n",
    "\n",
    "En ciencia de datos, hay un énfasis excesivo en el aprendizaje automáticoo aprendizaje profundo, y especialmente entre los científicos de datos jóvenes, la creencia de que trabajar en aislamiento solitario para maximizar la (s) puntuación (es) de precisión del modelo en un conjunto de datos de prueba es la definición de éxito. Este comportamiento es fomentado por cursos de capacitación, artículos en línea y especialmente Kaggle. \n",
    "\n",
    "__La precisión alta de la predicción del conjunto de pruebas me parece una interpretación extraña del éxito. Por lo general, en mi experiencia, es mejor probar diez escenarios de solución en lugar de pasar semanas en la optimización prematura de una única solución de problema porque no sabe de antemano qué va a funcionar.__ \n",
    "\n",
    "\n",
    "Cuando recibe comentarios de los consumidores y mide los resultados, ve si ha impulsado acciones beneficiosas o incluso un aprendizaje útil. En ese momento, puede decidir el valor de realizar más esfuerzos para optimizar.El objetivo debe ser conseguir un producto mínimo viable en producción. Un modelo perfecto en una computadora portátil que nunca entra en producción desperdicia esfuerzo, por lo que es peor que un modelo que no existe. \n",
    "\n",
    "Hay dominios en los que la precisión del modelo es primordial, como la precisión del diagnóstico médico, la detección de fraudes y AdTech, pero estos son una minoría en comparación con las aplicaciones en las que hacer cualquier cosa es una mejora significativa sobre no hacer nada. Incluso en los dominios que se benefician de manera desproporcionada de la optimización de la precisión del modelo, cuantificar el impacto en el mundo real es aún más importante.\n",
    "\n",
    "__Poner un modelo en producción requiere habilidades técnicas diferentes a la creación del modelo. Las más importantes son las habilidades de desarrollo de software relevantes. Para muchos científicos de datos, que principalmente no tienen experiencia en desarrollo de software, la codificación es solo un medio para un fin. No saben que la codificación y la ingeniería de software son disciplinas con su propio conjunto de mejores prácticas.__\n",
    "\n",
    "__Alternativamente, si lo saben, tienden a ver la escritura de código reutilizable , el control de versiones y las pruebas o la documentación como obstáculos que deben evitarse.__\n",
    "\n",
    "__Las habilidades de desarrollo débiles causan dificultades, especialmente para la reproducibilidad, el desempeño y la calidad del trabajo. Las barreras para que los modelos entren en producción no son responsabilidad exclusiva de los científicos de datos.__\n",
    "\n",
    "\n",
    "__A menudo, no tienen acceso a las herramientas y sistemas necesarios para que sus modelos entren en producción y, por lo tanto, deben confiar en otros equipos para facilitar la implementación. Los científicos de datos ingenuos ignoran el abismo entre el desarrollo local y la producción basada en servidor y lo tratan como un problema de arrojarlo por encima del cerco al no pensar en las implicaciones de su elección de lenguaje de programación. Esta inexperiencia provoca fricciones y fallas evitables.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brecha de conocimiento de TI\n",
    "\n",
    "\n",
    "Superficialmente, la ciencia de datos y el desarrollo de software comparten similitudes. Ambos involucran códigos, datos, bases de datos y entornos informáticos. Entonces, los científicos de datos requieren algunas habilidades de desarrollo de software.\n",
    "\n",
    "El entrenamiento de modelos es iterativo y computacionalmente costoso. La memoria de alta capacidad y las CPU más potentes le permiten utilizar más datos y algoritmos sofisticados. Los lenguajes y las bibliotecas que se utilizan para crear modelos de aprendizaje automático están especializados para el análisis de datos, normalmente los lenguajes de programación R y Python y sus bibliotecas y paquetes, respectivamente. Sin embargo, una vez que se ha creado un modelo, los procesos de implementación son mucho más familiares para un desarrollador de software.\n",
    "\n",
    "\n",
    "En la programación regular , la lógica es la parte más importante. Es decir, asegurarse de que el código sea correcto es fundamental. Los entornos de desarrollo y prueba a menudo no necesitan computadoras de alto rendimiento, y los datos de muestra con cobertura suficiente son suficientes para completar las pruebas. En la ciencia de datos, tanto los datos como el código son fundamentales. No hay una respuesta correcta para probar, solo un nivel aceptable de precisión. A menudo, se requiere un código mínimo (en comparación con la programación regular) para ajustar y validar un modelo con una alta precisión de prueba (por ejemplo, 95%). La complejidad radica en garantizar que los datos estén disponibles, se comprendan y sean correctos.\n",
    "\n",
    "\n",
    "Las necesidades de los científicos de datos a menudo son malinterpretadas por TI, incluso por aquellos que brindan apoyo como \"agradables\". Con frecuencia se les pide a los científicos de datos que justifiquen por qué necesitan acceso a múltiples fuentes de datos, datos de producción completos, software específico y computadoras poderosas cuando otros \"desarrolladores\" no los necesitan y los analistas de informes han \"extraído\" datos durante años simplemente ejecutando SQL consultas sobre bases de datos relacionales. TI está frustrado porque los científicos de datos no comprenden las razones detrás de las prácticas de TI. Los científicos de datos se sienten frustrados porque no es fácil justificar el valor de lo que consideran necesidades por adelantado. Más de una vez la pregunta \"¿Pero por qué necesita estos datos?\" ha hecho que mi corazón se hunda.\n",
    "\n",
    "\n",
    "Es raro ver procesos de TI diseñados para soportar procesos analíticos avanzados. Comienza con la forma en que se capturan los datos. Muchos desarrolladores ven la captura de nuevos elementos de datos como una carga con un costo asociado en el tiempo de planificación, análisis, diseño, implementación y mantenimiento. La mayoría de las organizaciones, por lo tanto, recopilan datos principalmente para respaldar procesos operativos como la gestión de relaciones con el cliente (CRM), la gestión financiera, la gestión de la cadena de suministro, el comercio electrónico y el marketing. Con frecuencia, estos datos residen en silos separados, cada uno con su estricta estrategia de gobierno de datos.\n",
    "\n",
    "\n",
    "\n",
    "A menudo, los datos pasarán a través de un ETL (Extract, Transform, Load) proceso para transformarla en datos estructurados (típicamente un formato tabular) antes de cargarlo en un almacén de datos para que sea accesible para el análisis. Hay algunos inconvenientes en este enfoque para la ciencia de datos. Solo un subconjunto de datos se abre paso a través del ETL donde generalmente se prioriza para los informes. Agregar nuevos elementos de datos puede llevar meses de desarrollo. Como tal, los datos sin procesar no están disponibles para los científicos de datos. ¡Los datos brutos son lo que necesitan!\n",
    "\n",
    "\n",
    "Los almacenes de datos tradicionales generalmente solo manejan datos estructurados en esquemas relacionales (con datos divididos en múltiples tablas combinables para evitar la duplicación y mejorar el rendimiento) y pueden tener dificultades para administrar la escala de datos que tenemos disponibles en la actualidad. Tampoco manejan casos de uso modernos que requieren datos no estructurados como texto o, a veces, incluso formatos de datos semiestructurados generados por máquina como JSON (notación de objetos JavaScript). Una solución es la creación de lagos de datos donde se almacenan los datos en el formato nativo de crudo y pasa por un ELT (extraer, de carga, Transform) proceso cuando sea necesario, con la transformación de ser dependiente del caso de uso.\n",
    "Cuando un lago de datos no está disponible, los científicos de datos deben extraer los datos ellos mismos y combinarlos en una máquina local o trabajar con ingenieros de datos para construir tuberías en un entorno con las herramientas, los recursos informáticos y el almacenamiento que necesitan. Las solicitudes de acceso a datos, entornos de aprovisionamiento e instalación de herramientas y software suelen ser responsabilidad de equipos separados con diferentes preocupaciones sobre seguridad, costos y gobernanza. Como tal, los científicos de datos deben trabajar con diferentes grupos para implementar y programar sus modelos, paneles y API. Con procesos implementados incongruentes con las necesidades de la ciencia de datos, los costos son muy elevados.\n",
    "\n",
    "\n",
    "\n",
    "Todo el ciclo de vida de los datos se divide entre muchos equipos de TI y cada uno de ellos toma decisiones racionales de forma aislada en función de sus objetivos funcionales de silos. Tales objetivos de silos no sirven a los científicos de datos. Para los científicos de datos que necesitan canalizaciones de datos desde los datos sin procesar hasta el producto de datos final, surgen desafíos importantes. Necesitan justificar sus requisitos y negociar con múltiples partes interesadas para completar una entrega. Incluso si tienen éxito, seguirán dependiendo de otros equipos para muchas tareas y estarán a merced de los retrasos y la priorización. Ninguna persona o función es responsable de todo el proceso, lo que provoca retrasos, cuellos de botella y riesgo operativo.\n",
    "\n",
    "\n",
    "La seguridad y la privacidad de los datos se mencionan ocasionalmente como obstáculos para evitar el acceso y el procesamiento de los datos. Existe una preocupación genuina por garantizar el cumplimiento de las regulaciones, respetar la privacidad del usuario, proteger la reputación, defender la ventaja competitiva y prevenir daños malintencionados. Sin embargo, estas preocupaciones también pueden usarse para tomar una ruta de aversión al riesgo y no implementar soluciones que permitan el uso seguro, legítimo y ético de los datos. Por lo general, los problemas ocurren cuando se implementan políticas de privacidad y seguridad de datos sin realizar un análisis exhaustivo de costo-beneficio y comprender completamente el impacto en el análisis de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
