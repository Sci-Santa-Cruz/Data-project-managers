{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La omisión de valores en el conjunto de datos puede tener diversos efectos y diferentes grados de impacto. En términos generales, se suelen considerar los siguientes grados de impacto, dependiendo del porcentaje de valores faltantes (*dumb rules*):\n",
    "La omisión de valores en el conjunto de datos puede tener diversos efectos y diferentes grados de impacto. En términos generales, se suelen considerar los siguientes grados de impacto, dependiendo del porcentaje de valores faltantes (*dumb rules*):\n",
    "\n",
    "* Menos de 1%: Trivial (no relevante)\n",
    "* 1-5%: Manejable\n",
    "* 5-15%: Manejable mediante métodos sofisticados\n",
    "* Más de 15%: Crítico, con impacto severo en cualquier tipo de interpretación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "El parámetro *tresh* en DataFrame.dropna() permite eliminar todos los renglones que no contengan al menos el número de columnas \"limpias\" expresado por el prámetro. En los ejemplos a continuación, se conservan 1) sólo los renglones que tienen al menos 8 columnas *limpias* y 2) los renglones que tienen al menos 7 columnas con valores definidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dropna(thresh=8), '\\n')\n",
    "print(df.dropna(thresh=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputación \n",
    "\n",
    "El análisis de casos completos es una opción aceptable si el porcentaje de valores faltantes es pequeño. En la mayoría de los casos, es preferible reemplazar los valores faltantes por valores calculados por omisión o valores calculados. Esta operación se denomina **imputación**. En el siguiente ejemplo, todos los valores *NaN* son reemplazados por 0, lo cual pudiera seguir la lógica de que \"si el dato no está disponible es que en realidad era cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3 = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, en muchos casos un valor por omisión de cero no tiene sentido. En nuestro ejemplo con los datos de diabetes, un valor de cero en la columna *pad* (*Presión diastólica de la sangre*) es imposible en una persona viva. En este caso, una mejor opción es rellenar los valores faltantes por el mínimo registrado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.fillna(df.min())\n",
    "df.fillna(df.mode())\n",
    "df.fillna(df.max())\n",
    "print(df3, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rellene los valores faltantes con los anteriores\n",
    "Otra alternativa común es rellenar los valores faltantes con el valor no nulo previo o el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df, \"\\n\")\n",
    "# this is equivalent to both method='ffill' and .ffill()\n",
    "print(\"Replicar hacia enfrente\\n\", df.fillna(method='pad'), \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Rellenar con el siguiente\n",
    "print(\"Replicar hacia atrás\\n\", df.fillna(method='bfill'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolación\n",
    "La interpolación es un método formal para estimar valores en una serie de datos. La idea consiste en suponer que todos los puntos en la serie se encuentran sobre una curva subyacente, aunque desconocida. \n",
    "La forma más simple de interpolación es la *lineal*. En este caso se parte de dos puntos conocidos y los puntos intermedios (faltantes) se calculan como si estuvieran colocados sobre la línea recta que une a los puntos conocidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "ser = pd.Series(np.arange(1, 10.1, .25) ** 2 + np.random.randn(37))\n",
    "\n",
    "missing = np.array([4, 13, 14, 15, 16, 17, 18, 20, 29])\n",
    "\n",
    "ser[missing] = np.nan\n",
    "\n",
    "methods = ['linear', 'quadratic', 'cubic']\n",
    "\n",
    "df = pd.DataFrame({m: ser.interpolate(method=m) for m in methods})\n",
    "\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalización\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_boundaries_std(df, variable, sigma):\n",
    "    \n",
    "    lower_boundary = df[variable].mean() - df [variable].std() * sigma\n",
    "    upper_boundary = df[variable].mean() + df [variable].std() * sigma\n",
    "    \n",
    "    return upper_boundary, lower_boundary\n",
    "\n",
    "\n",
    "\n",
    "def find_boundaries_IQR (df, variable, distance): \n",
    "\n",
    "    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25) \n",
    "\n",
    "    lower_boundary = df[variable].quantile (0.25) - (IQR * distance) \n",
    "    upper_boundary = df[variable].quantile (0.75) + (IQR * distancia) \n",
    "\n",
    "    return upper_boundary, lower_boundary\n",
    "\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n",
    "\n",
    "Normalization - Standardization (Z-score scaling) \tremoves the mean and scales the data to unit variance.\n",
    "z = (X - X.mean) / std\n",
    "\n",
    "\n",
    "\n",
    "Robust scaling \tremoves the median and scales the data according to the quantile range (defaults to IQR)\n",
    "X_scaled = (X - X.median) / IQR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo de optimización FTRL\n",
    "\n",
    "Los modelos lineales de dimensiones altas se benefician del uso de una variante de optimización basada en gradientes, que se denomina FTRL. Este algoritmo tiene la ventaja de ajustar la tasa de aprendizaje de distintas maneras para diferentes coeficientes, lo cual puede resultar útil si algunos atributos raramente toman valores distintos de cero (también es adecuado cuando se utiliza regularización L1). Podemos aplicar FTRL a través de la función FtrlOptimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
